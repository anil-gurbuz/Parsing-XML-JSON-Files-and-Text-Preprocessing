{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parse 29628792.dat File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all i want to read the data file into a python variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('29628792.dat','r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then i want to differentiate every job advirtesment. These advirtesements are seperated by successive mineses (-).\n",
    "Therefore, i can use split() function on data variable after changing it into a string because it is a .TextIOWrapper type variable.\n",
    "\n",
    "To do that i can use readlines() and get every single line and save them into a list of strings then i can split this list using entries that are consist of just successive mineses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_data=data.readlines()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1623590"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know that there are 1,623,590 lines in our data file. And every job advertisement are seperated with each other by a line that are consist of only successive minuses (-).\n",
    "        Therefore, we can concatanate lines between minuse signs then we will have list of strings that every string represents a job advertisement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = []\n",
    "string=''\n",
    "\n",
    "for lines in new_data:\n",
    "    if lines=='------------------------------\\n':\n",
    "        jobs.append(string)\n",
    "        string=''\n",
    "    else:\n",
    "        string += lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32764"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list with 32,764 strings inside it and every string represents a job listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am expecting to have 11 different attributes in every job advertisment.( If the dataset doesn't have missing information or nulls in it.)\n",
    "\n",
    "These are;\n",
    "* Listing ID\n",
    "* Job Title\n",
    "* Location\n",
    "* Application Deadline\n",
    "* Start Date\n",
    "* Company Information\n",
    "* Application Procedure\n",
    "* Salary\n",
    "* Needed Qualifications\n",
    "* Job Responsibilities\n",
    "* Listing Description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture these attributes, we need to use regular expression that search for the expressions that are located at the begininning of the line( which means start with /n) and end with : and space. (There can be exceptions but i think this is a good point to start with.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Additionaly, we know that there are 11 attributes so if there are more or less atrributes in any job posting, it can be a hint for me to understand exceptions and NA's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, i will write a for loop to take every job posting one by one and capture attributes then create a dictionary with these attributes and their corresponding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=[]\n",
    "pattern=re.compile(r\"^[\\w]+[ ]?[\\w]*:\")\n",
    "\n",
    "\n",
    "for each in sample:\n",
    "    a=re.findall(r\"^[\\w]+[ ]?[\\w]*:\",each,re.M)\n",
    "    keys.append(a)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regex used above matches with expressions that starts with any alphanumeric character or an underscore then may have a space and then may have zero or more alphanumeric character or underscore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the attributes of every job posting but we have extra keys which are not really an attribute. In order to get ride of these extra keys, i will consider to write unique regular expression for every attribute using key words for every attribute. For example for company information, there should be (comp, info, about, company, inf ... ) so i can use this kind of key words for all attributes and if there is a key that doesn't fits in any of these keywords i can drop it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Listing ID      - Must include [id] \n",
    "* Job Title   - Must include one of ['ttl' , 'title'] or start with job then _ or space then ['t''i''t''e'] one to 4 of them.\n",
    "* Location  - Must include 'loc' or start with 'address' and letters then ',' letters and ',' again.\n",
    "* Application Deadline  - Must include 'dead' or start with 'app' then [lications] one to all of them then there may be an _ or space then a 'd'.\n",
    "* Start Date - Must include one of ['start','strt'] or start with 's' then _ or space then a 'd'. Another option is just 'dates'.\n",
    "* Company Information - Must include one of ['inf','comp','about']\n",
    "* Application Procedure  - Must include one of ['prc','pro'] or start with 'app' then [lications] one to all of them then there may be an _ or space then a 'p'.\n",
    "* Salary - Must include one of ['sal','rem']\n",
    "* Needed Qualifications - Must include one of ['req','qua','needed']\n",
    "* Job Responsibilities - Must include 'resp'\n",
    "* Listing Description = Must include 'des'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_capture = r\"id\"\n",
    "title_capture = r\"titl|ttl|title|job[_ ]*[tite]{1,4}\"\n",
    "loc_capture = r\"loc[ation]{0,5}|address[: ][/w]+,[/w]+,\"\n",
    "deadline_capture = r\"dead|app[lications]{0,9}[_ ]*d\"\n",
    "start_capture = r\"start|strt|s[_ ]*d|dates\"\n",
    "info_capture = r\"inf|comp|about\"\n",
    "procedure_capture = r\"prc|pro|app[lications]{0,9}[_ ]*p\"\n",
    "salary_capture = r\"sal|rem\"\n",
    "qual_capture = r\"req|qua|needed\"\n",
    "resp_capture = r\"resp\"\n",
    "desc_capture = r\"des\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for job in range(len(keys)):\n",
    "    for attr in range(len(keys[job])-1,-1,-1):\n",
    "        \n",
    "        \n",
    "        if re.search(id_capture,keys[job][attr],re.I) != None :\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        elif re.search(title_capture,keys[job][attr],re.I) != None:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        elif re.search(loc_capture,keys[job][attr],re.I) != None:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        elif re.search(deadline_capture,keys[job][attr],re.I) != None:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        elif re.search(start_capture,keys[job][attr],re.I) != None:\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        elif re.search(info_capture,keys[job][attr],re.I) != None:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        elif re.search(procedure_capture,keys[job][attr],re.I) != None:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        elif re.search(salary_capture,keys[job][attr],re.I) != None:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        elif re.search(qual_capture,keys[job][attr],re.I) != None:\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        elif re.search(resp_capture,keys[job][attr],re.I) != None:\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        elif re.search(desc_capture,keys[job][attr],re.I) != None:\n",
    "            continue\n",
    "            \n",
    "\n",
    "        \n",
    "        del keys[job][attr]\n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have got rid of wrong attributes and ready to find the values of attributes and create a list of dictionaries including every job posting as an element of list. The regular expressions below captures the values of every attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "values=[]\n",
    "\n",
    "for i in range(len(keys)):\n",
    "    temporary_list=[]\n",
    "    for j in range(len(keys[i])-1):\n",
    "        temporary_list.append(re.findall(r\"%s(.*?)\\n%s\" %(keys[i][j],keys[i][j+1]) , sample[i],re.S))\n",
    "    \n",
    "    temporary_list.append(re.findall(r\"%s(.*)\"%keys[i][j+1], sample[i], re.M ))\n",
    "    \n",
    "    values.append(temporary_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunk of code below is just to make sure that there are exactly the same number of attributes and their corresponding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(keys)):\n",
    "    if len(keys[i]) != len(values[i]):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 2 lists of strings, one of them has the attributes and the other one has the corresponding values of the attributes. ( Both of them are in the same order! )\n",
    "\n",
    "Next step is to change the attribute names to make them steady for all job postings and labelling the missing values as N/A.\n",
    "\n",
    "To do that, i will use the same regexes that i used above to capture all atributes one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in range(len(keys)):\n",
    "    for attr in range(len(keys[job])-1,-1,-1):\n",
    "        \n",
    "        \n",
    "        if re.search(id_capture,keys[job][attr],re.I) != None :\n",
    "            keys[job][attr]='ID'\n",
    "        \n",
    "        \n",
    "        elif re.search(title_capture,keys[job][attr],re.I) != None:\n",
    "            keys[job][attr]='Title'\n",
    "        \n",
    "        \n",
    "        elif re.search(loc_capture,keys[job][attr],re.I) != None:\n",
    "            keys[job][attr]='Location'\n",
    "        \n",
    "        \n",
    "        elif re.search(deadline_capture,keys[job][attr],re.I) != None:\n",
    "            keys[job][attr]='Application_Deadline'\n",
    "        \n",
    "        \n",
    "        elif re.search(start_capture,keys[job][attr],re.I) != None:\n",
    "            keys[job][attr]='Start_Date'\n",
    "            \n",
    "        \n",
    "        elif re.search(info_capture,keys[job][attr],re.I) != None:\n",
    "            keys[job][attr]='About_Company'\n",
    "        \n",
    "        \n",
    "        elif re.search(procedure_capture,keys[job][attr],re.I) != None:\n",
    "            keys[job][attr]='Application_Procedure'\n",
    "        \n",
    "        \n",
    "        elif re.search(salary_capture,keys[job][attr],re.I) != None:\n",
    "            keys[job][attr]='Salary'\n",
    "        \n",
    "        \n",
    "        elif re.search(qual_capture,keys[job][attr],re.I) != None:\n",
    "            keys[job][attr]='Qualifications'\n",
    "            \n",
    "        \n",
    "        elif re.search(resp_capture,keys[job][attr],re.I) != None:\n",
    "            keys[job][attr]='Responsibility'\n",
    "            \n",
    "        \n",
    "        elif re.search(desc_capture,keys[job][attr],re.I) != None:\n",
    "            keys[job][attr]='Description'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i need to get rid of repeating attributes because my regex may catch more than one times the same attribute and there can be repeating attributes in the dataset as well. I can do it just by changing these 2 lists into a list of dictionaries and if there is a repeating attribute ( which will be a key value in dictionaries) the second appeareance of it will be taken into account and first one will be deleted because my loop assigns the last value with the same key below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dict=[]\n",
    "for i in range(len(keys)):\n",
    "    temporary_dict={}\n",
    "    for j in range(len(keys[i])):\n",
    "        temporary_dict[keys[i][j]]=values[i][j]\n",
    "    \n",
    "    list_of_dict.append(temporary_dict)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, i need to deal with the N/A's. There are some job postings that has missing values which means in dictionary of each posting that has key values less than 11 , has a missing key value.\n",
    "\n",
    "Therefore, i will find the missing key values in every dictionary and add these keys and their corresponding value which is NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_of_dict)):\n",
    "    \n",
    "    all_attr=['ID', 'About_Company', 'Description', 'Responsibility', 'Qualifications', 'Application_Deadline', 'Title', 'Application_Procedure', 'Salary', 'Location', 'Start_Date']\n",
    "    \n",
    "    for j in list_of_dict[i]:\n",
    "        a=all_attr.index(j)\n",
    "        del all_attr[a]\n",
    "    \n",
    "    if len(all_attr) !=0 :\n",
    "        for k in all_attr:\n",
    "            list_of_dict[i][k]=['NA']\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i will check is there a repeating ID's and if there is i will delete the whole job posting of the second appearance of repeating ID in the List of dictionary structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=[]\n",
    "del list_of_dict[0]\n",
    "for job in range(0,len(list_of_dict)):\n",
    "    ids.append(list_of_dict[job]['ID'][0])\n",
    "\n",
    "\n",
    "\n",
    "ids=set(ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32763"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in range(len(list_of_dict)-1,-1,-1):\n",
    "    if list_of_dict[job]['ID'][0] in ids:\n",
    "        ids.remove(list_of_dict[job]['ID'][0])\n",
    "        \n",
    "    elif list_of_dict[job]['ID'][0] not in ids:\n",
    "        del list_of_dict[job]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26805"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at length of list_of_dict variable whose every entry (dictionary) represents a job posting, there were about 6000 repeating ID's in dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the chunk of code below, i got ride of the new line characters to make the content of the structure appropriate and more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attr=['ID', 'About_Company', 'Description', 'Responsibility', 'Qualifications', 'Application_Deadline', 'Title', 'Application_Procedure', 'Salary', 'Location', 'Start_Date']\n",
    "\n",
    "for job in range(len(list_of_dict)):\n",
    "    for att in all_attr:\n",
    "        list_of_dict[job][att]=list_of_dict[job][att][0].strip()\n",
    "        list_of_dict[job][att]=list_of_dict[job][att].replace('\\\\\\n',' ')\n",
    "        list_of_dict[job][att]=list_of_dict[job][att].replace('\\\\n',' ')\n",
    "        list_of_dict[job][att]=list_of_dict[job][att].replace('\\n',' ')\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Changing into JSON and Creating Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "dictionary = {}\n",
    "\n",
    "\n",
    "for job in list_of_dict:\n",
    "    dictionary['listing']=job\n",
    "    new_list.append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "final={}\n",
    "final['listings']=new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('29628792.json', 'w') as output:\n",
    "    json.dump(final,output, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 4. Changing into XML and Creating Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml='<?xml version=\"1.0\" encoding=\"UTF-8\" ?> \\n<listings>'\n",
    "\n",
    "for job in list_of_dict:\n",
    "    xml+='\\t<listing id=' +\"'\" + job['ID'] + \"'>\" + '\\n' +'\\t\\t'\n",
    "    xml+= '<title>' + job['Title'] + '</title>' + '\\n' +'\\t\\t'\n",
    "    xml+= '<location>' + job['Location'] + '</location>' + '\\n' +'\\t\\t'\n",
    "    xml+= '<job_descriptions>' + '\\n' +'\\t\\t\\t' \n",
    "    xml+= '<description>' + job['Description'] + '</description>' +'\\n' +'\\t\\t'\n",
    "    xml+= '</job_descriptions>' + '\\n' + '\\t\\t'\n",
    "    xml+= '<job_responsibilities>' + '\\n' +'\\t\\t\\t' \n",
    "    xml+= '<responsibility>' + job['Responsibility'] + '</responsibility>' + '\\n' + '\\t\\t'\n",
    "    xml+= '</job_responsibilities>' + '\\n' + '\\t\\t'\n",
    "    xml+= '<required_qualifications>' + '\\n' +'\\t\\t\\t'\n",
    "    xml+= '<qualification>'  + job['Qualifications'] + '</qualification>' + '\\n' + '\\t\\t'\n",
    "    xml+= '</required_qualifications>'  + '\\n' + '\\t\\t'\n",
    "    xml+= '<salary>' + job['Salary'] + '</salary>'  + '\\n' + '\\t\\t'\n",
    "    xml+= '<application_procedure>' + job['Application_Procedure'] + '</application_procedure>' + '\\n' + '\\t\\t'\n",
    "    xml+= '<start_date>' + job['Start_Date'] + '<start_date>' + '\\n' + '\\t\\t'\n",
    "    xml+= '<application_deadline>' +job['Application_Deadline'] +'</application_deadline>' + '\\n' + '\\t\\t'\n",
    "    xml+= '<about_company>' + job['About_Company'] + '</about_company>' + '\\n' +'\\t'\n",
    "    xml+= '</listing>'\n",
    "\n",
    "    \n",
    "xml+='\\n</listings>'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('29628792.xml', 'w') as output2:\n",
    "    output2.write(xml)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
