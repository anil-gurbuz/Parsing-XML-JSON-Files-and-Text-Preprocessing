{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 2 in Assessment 1\n",
    "#### Student Name: Anil Gurbuz\n",
    "#### Student ID: 29628792\n",
    "\n",
    "Date: XX/XX/XXXX\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 2.7.11 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "* pandas ( included in Anaconda Python 2.7) \n",
    "* re (for regular expression, included in Anaconda Python 2.7)\n",
    "* nltk.stem.porter\n",
    "* sklearn.feature_extraction.tex\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Reading Resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "resumeno=[  457, 119, 612 , 391, 695, 245, 730, 607, 474, 338, 382, 10, 481, 339, 584, 820, 722, 610,\n",
    " 858,  13, 237, 109, 537, 523, 331, 429, 293, 277, 437, 459, 432, 452, 197, 522, 317, 706,\n",
    "  44, 770, 388, 493,  77, 503, 218, 774, 363, 126, 687, 666, 771, 672,  41, 711, 684, 614,\n",
    " 123, 232, 590,  38, 523,  15, 566, 444, 559,   8, 377, 692, 591, 403, 268,  63, 552, 859,\n",
    " 333, 645, 468, 372, 143, 853, 146, 669,  76, 250, 457, 704, 761, 803, 244, 751, 472, 293,\n",
    "  47, 389, 278, 610, 679, 416, 165, 106, 479, 763, 261, 739, 746, 494, 356, 522, 100, 673,\n",
    " 825,  22, 568, 302, 598, 459, 130, 699, 504, 630, 509, 378, 167, 740, 742, 472, 550, 834,\n",
    " 384, 772, 242, 431, 845, 414, 640, 227, 488, 821, 188, 661, 838, 705,  58, 207, 493,   8,\n",
    " 528, 571, 824, 584, 721, 192, 118, 241, 113, 712, 568, 178, 323, 309, 118, 576, 312, 175,\n",
    " 272, 398,  42, 634, 608, 860, 835, 859, 325, 818,  17, 640, 799, 726, 624, 160, 650, 759,\n",
    " 360, 115,  17, 428,  64, 542, 210, 688, 454, 194, 788, 789, 247, 397, 464, 782, 609, 508,\n",
    "  33, 611, 571,  97, 514, 286, 350, 648, 482,  35, 394, 203, 665, 733, 285, 354, 344,  59,\n",
    " 215, 483, 450, 707, 454, 282, 107,  96, 829, 594, 429, 646,  69, 273, 635, 323, 160, 308,\n",
    " 508, 522, 493, 586, 860, 300, 465, 458, 774,  85, 849, 162, 688, 144, 503, 519]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumes are read by using regex and resumes list created above to specify the name of the resume files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_resumes=[]\n",
    "\n",
    "\n",
    "for each in resumeno:\n",
    "    data = open(r\"resumeTxt/resume_(%s).txt\" %each)\n",
    "    list_resumes.append(data.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the stopwords and structuring it to use in the upcoming steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=open('stopwords_en.txt')\n",
    "stopwords=stopwords.readlines()\n",
    "\n",
    "real_stopwords=[]\n",
    "\n",
    "for i in range(len(stopwords)):\n",
    "    stopwords[i]=stopwords[i].replace('\\n','')\n",
    "    \n",
    "    if len(stopwords[i]) > 1:\n",
    "        real_stopwords.append(stopwords[i])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_resumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list_resumes is a list that has 250 entries and every entry is a string containing an entire resume. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step, i will tokenize the resumes using the regex , specified at task2 part of the assignment.\n",
    "Then i create a list of list ( resumes) which includes 250 lists inside ( like list_resumes variable) and every list has tokens of its resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes=[]\n",
    "\n",
    "for resume in list_resumes:\n",
    "    tokens = re.findall(r\"\\w+(?:[-']\\w+)?\", resume)\n",
    "    resumes.append(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming - Stop words removal - digit removal - less than 2 char-token removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step, \n",
    "\n",
    "* To remove the tokens containing digits, a simple regex used to find the digit in every token and if there is a digit in the token, removed.\n",
    "\n",
    "* For stemming, each resume is taken using for loop from resumes variable than every token in the each resume is taken again with for loop.\n",
    "        * First these tokens are stemmed using PorterStemmer.\n",
    "        * Then length of these stemmed tokens are checked and if it is less than 3, removed.\n",
    "        * Then stemmed tokens checked again to make sure they are not in the stop words list.\n",
    "        * Lastly again each token checked whether they include a digit or not, if so removed.\n",
    "* Then a temp list is created for every resume and if a token passes from every criterias defined above, added to that list to be added to another list in loop\n",
    "\n",
    "* Last step is to add these temporary lists to a new list which is named as \"token_stemmed\". This list again contains lists whose content is the all appropriate tokens for a resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_stemmed=[]\n",
    "no_digits = re.compile(r'\\d+', re.M)\n",
    "\n",
    "for resume in resumes:\n",
    "    temp=[]\n",
    "    \n",
    "    for token in resume:\n",
    "        stemmed_word = PorterStemmer().stem(token)\n",
    "        if (len(stemmed_word)> 2) and (stemmed_word not in real_stopwords):\n",
    "            if not re.match(no_digits, stemmed_word):\n",
    "                temp.append(stemmed_word)\n",
    "    \n",
    "    token_stemmed.append(temp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prepare our data structure for the CountVectorizer function, i created a list of string. Every string's content is the all appropriate tokens of corresponding resume of string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "resumes = []\n",
    "for doc in token_stemmed:\n",
    "    resume = ' '.join(doc)\n",
    "    resumes.append(resume)\n",
    "    \n",
    "print(len(resumes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment, rare tokens and the tokens has the frequency higher than 98% is removed using CountVectorizer fuinction and 200 most meaningfull bigrams are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneword = CountVectorizer(analyzer='word',\n",
    "                        ngram_range=(1,1),\n",
    "                        # tokens must appear in more than 2 percent of the documents\n",
    "                       min_df=0.02,\n",
    "                        # tokens\n",
    "                       max_df=0.98)\n",
    "\n",
    "twoword = CountVectorizer(analyzer='word',\n",
    "                # tokens must appear in more than 2 percent of the documents\n",
    "               min_df=0.02,\n",
    "                # tokens\n",
    "               max_df=0.98,\n",
    "               ngram_range=(2,2),\n",
    "                max_features=200,\n",
    "                tokenizer=None,\n",
    "                preprocessor=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Output Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below generates the sparse representation of the resumes and creates the output with specified format.\n",
    "Ordered in alphabetic order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse1 = oneword.fit_transform(resumes)\n",
    "sparse2 = twoword.fit_transform(resumes)\n",
    "\n",
    "unigrams = oneword.get_feature_names()\n",
    "bigrams = twoword.get_feature_names()\n",
    "vectors = unigrams + bigrams\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectors.sort()\n",
    "\n",
    "output1=''\n",
    "\n",
    "for index, vector in enumerate(vectors):\n",
    "    line = f'%s : %s' %(index, vector)\n",
    "    output1 += line + '\\n'\n",
    "    \n",
    "with open('29628792_vocab.txt', 'w') as vocab:\n",
    "    vocab.write(output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First chunk of code, creates two dataframes having content of unigrams-bigrams(respectively) and their frequencies in every resume.\n",
    "\n",
    "Then vector_rep variable is the combined dataframe of these 2 dataframes whose columns are sorted alphabeticly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_df = pd.DataFrame(sparse1.todense(), columns=unigrams)\n",
    "# bigram_df =\n",
    "bigram_df = pd.DataFrame(sparse2.todense(), columns=bigrams)\n",
    "\n",
    "vector_rep = pd.concat([unigram_df, bigram_df], axis=1).sort_index(axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below produces the output in the specified format. First uses a for loop to reach the which resume is considered then prints the resume number using the firstly created resumeno list variable. Then another for loop takes every column name to look at every values for every resume's every token. If the value is not zero, adds the vector index ( from vectors variable) and the corresponding frequency from(vector_rep variable). At the end writes the whole string in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "string=''\n",
    "for resume_no in range(len(vector_rep.index)) :\n",
    "    string+='resume_(%s).txt, ' %resumeno[resume_no]\n",
    "    for colmn in vector_rep.columns:\n",
    "        if vector_rep[colmn][resume_no] !=0:\n",
    "            string+= str(vectors.index(colmn)) + ':' + str(vector_rep[colmn][resume_no]) + ', '\n",
    "    string = string[:-2]\n",
    "    string+='\\n'\n",
    "\n",
    "\n",
    "    \n",
    "with open('29628792_countVec.txt','w') as countvec:\n",
    "    countvec.write(string)\n",
    "                \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
